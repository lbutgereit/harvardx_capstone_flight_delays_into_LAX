---
title: "Harvardx Capstone - Domestic Flight Delays into LAX"
author: "Laura Butgereit"
date: "08/05/2021"
output: 
  bookdown::pdf_document2:
    number_sections: true
    toc: true
    lof: yes
    keep_tex: true
    extra_dependencies: ["float"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.pos="H", fig.height=6, fig.width=6, fig.align="center",
                      out.extra="")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(Metrics)) install.packages("Metrics", repos = "http://cran.us.r-project.org")
library(tidyverse)
library(lubridate)
library(stringr)
library(dplyr)
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
```

```{r, echo=FALSE}
#
# this is the theme for the graphs and a few functions to
# ensure that all the graphs look similar
#
# theme from https://emanuelaf.github.io/own-ggplot-theme.html
#
my_theme <- function() {
  theme(
    # add border 1)
    panel.border = element_rect(colour = "deepskyblue4", fill = NA, linetype = 1),
    # color background 2)
    panel.background = element_rect(fill = "aliceblue"),
    # modify grid 3)
    panel.grid.major.x = element_line(colour = "deepskyblue4", linetype = 3, size = 0.5),
    panel.grid.minor.x = element_blank(),
    panel.grid.major.y =  element_line(colour = "deepskyblue4", linetype = 3, size = 0.5),
    panel.grid.minor.y = element_blank(),
    # modify text, axis and colour 4) and 5)
    axis.text = element_text(colour = "deepskyblue4"),
    axis.title = element_text(colour = "deepskyblue4"),
    axis.ticks = element_line(colour = "deepskyblue4"),
    # legend at the bottom 6)
    legend.position = "top",
    #plot.fill = "deepskyblue4",
    plot.title = element_text(colour = "deepskyblue4", hjust = 0.5),
  )
}
update_geom_defaults("bar", list(fill = "deepskyblue4", alpha = 0.6, color = "deepskyblue4"))   # color was blue
update_geom_defaults("point", list(fill = "deepskyblue4", alpha = 0.6, color = "deepskyblue4"))  # color was blue
update_geom_defaults("smooth", list(fill = "deepskyblue4", alpha = 0.6, color = "violet"))
update_geom_defaults("boxplot", list(fill = "deepskyblue4", alpha = 0.6, outlier.color = "violet"))

#
# this function is to ensure that all histograms 
# have a similar look and feel
#
#
histogram_graph <- function(data, x, bins=10) {
  p <- ggplot(aes(x), data=data) +
    #scale_y_continuous()+
    scale_y_log10() +
    #geom_histogram(bins=bins)  +
    geom_histogram(binwidth=30) +
    #scale_x_log10() +
    ylab("Quantity") +
    my_theme()
  p
}

#
# this function is to ensure that all graphs which
# have rating on the y axis have the same scale etc
#
# this will be used by all the graphs which have
# average ratings on the y axis
#
rating_graph <- function(data, x, y) {
  p <- ggplot(aes(x, y), data=data) +
    scale_y_continuous(limits=c(0.0, 5.5), breaks = 0:5)+
    geom_point() + 
    geom_smooth(method = "loess", formula="y ~ x") +
    ylab("Average Rating") +
    my_theme()
  p
}

#
#
#
bar_graph <- function(data, x, y) {
  p <- ggplot(aes(x, y), data=data) +
    geom_bar(stat = "identity") +
    my_theme()
  p
}

#
# boxplot function to ensure all boxplots looks
# similar
#
boxplot_graph <- function(data, x) {
 p <- ggplot(aes(x, y), data=data) +
    geom_boxplot() +
    my_theme()
 p
}

```

\listoffigures

\listoftables
\newpage

# Introduction

Wada, wada, wada,

This project was part of a Capstone project in Harvardx's *Professional Certificate in Data Science*.

Section \@ref(environment) describes the computational environment where this
research was executed.  This section also itemizes any modifications that might
have needed to be done in order to process the large data files.

Section \@ref(download) describes issues which were encountered in downloading the
required data files.  This section provides various strategies which were
utilized to handle these issues. 

Section \@ref(exploration) of this paper describes the exploration into the dataset.
This section includes numerical values and some histograms of distributions.
After the initial exploration, the training dataset was reshaped to expose
more information. 

Section \@ref(model) describes the various investigations into possible
models to be able to predict the ratings given to a movie.  There were a
number of different strategies which were investigated.  Each model is
described along with the RMSE (root mean square error) for that model.


Conclusions can be found in Section \@ref(conclusions).

Before continuing with this document, the author wishes to share the wise words of Donald Knuth (1984).  In his journal
paper entitled *Literate Programming* he states:

\begin{quotation}
Let us change our traditional attitude to the construction of [computer] programs:  Instead of imagining that our 
main task is to instruct a 
\begin{em}
computer
\end{em}
what to do,  let us concentrate rather on explaining to 
\begin{em}
human beings
\end{em}
what we want a computer to do. 

The practitioner of literate programming can be regarded as an essayist,  whose  main concern  is with 
exposition and  excellence  of style.  
Such an author, with thesaurus in hand, chooses the names of variables carefully and explains what each 
variable means.  He or she strives
for a program that is comprehensible because its concepts have been introduced in an order that is best for human
understanding...
\end{quotation}

The author eschews variables such as *mu* and *y_hat* (which may be familiar to data science practitioners and
statisticians), in favour of more descriptive variables such as *overall_average* and *predicted_rating*.  Although this practice may
be in conflict with code examples in (Irizarry, 2021), the author is employed in the IT industry and is attending this
course with the view of integrating R (running in an automated fashion started by Java programs) into a Spring Boot environment.  As such the practices of good
naming conventions in source code are well ingrained.

\newpage
# Compute Environment and Memory issues {#environment}

The analyses presented in the paper were done on a Dell XPS 13 9380 13.3" Core i7-8565U Notebook. The notebook has 8GB memory and a Core i7-8565U processor.  The notebook was running Ubuntu 20.04.2 LTS with R 4.0.3 installed.

## Adding more swap space

There were memory issues and an additional 4G swap space was configured on the notebook with the following Linux commands executed as superuser:

```{r, eval=FALSE, echo=TRUE}
fallocate -l 4G /harvardx_swapspace
chmod 600 /harvardx_swapspace
mkswap /harvardx_swapspace
swapon /harvardx_swapspace
```

In order to have this swapspace available automatically when the notebook boots up
an entry needed to be made in the file

```{r, eval=FALSE, echo=TRUE}
/etc/fstab
```

which looks like

```{r, eval=FALSE, echo=TRUE}
/harvardx_swapfile none swap sw 0 0
```

## Modifying *swappiness*

The *swappiness* value defines how aggressively the Linux kernel swaps memory pages to swap devices.  The value ranges
between zero and one hundred (Ljubuncic, 2015).  A higher *swappiness* value implies a stronger preference toward
swapping more readily.  A lower value implies not swapping (Love, 2013).  The value can be configured on this file

```{r, eval=FALSE, echo=TRUE}
/etc/sysctl.conf
```

The entry looks like

```{r, eval=FALSE, echo=TRUE}
vm.swappiness=10
```

This change was made to stop Linux swapping excessively.

## Request extra memory when executing R

In order to successfully build and execute, the scripts were run in R and not Rstudio with the following
command

```{r, eval=FALSE, echo=TRUE}
R -max-mem-size=7000M --vanilla < Butgereit-script.R  > output.txt
```

The report was generated with the following command
```{r, eval=FALSE, echo=TRUE}
Rscript -e "rmarkdown::render('Butgereit-report.Rmd')"
```

\newpage
# Downloading, unzipping, and validating data {#download}

The author lives in a rural area of Africa where download speeds are slow and connections often break.  For that reason, the code supplied by the Edx website for downloading data was modified slightly to include a *timeout* parameter.  *The code reviewer may need to increase the parameter depending on his or her local circumstances.*  The code then attempted to download 
the required zip file, store the file locally, and then to check the size of the downloaded file to verify that the entire data file was downloaded.  If the download and size check did not succeed, error messages are printed and the script is stopped.  If the download and size check did succeed, only then is the data unzipped.

## Set up pseudo-constants

First a handful of pseudo-constants were set up

```{r, echo=TRUE}
download_timeout <- 600 # seconds == 10 minutes
on_time_performance_url <- 
"https://dax-cdn.cdn.appdomain.cloud/dax-airline/1.0.1/airline_2m.tar.gz"
on_time_performance_gz <- "airline_2m.tar.gz"
on_time_performance_gz_size <- 151681776
on_time_performance_csv <- "airline_2m.csv"
on_time_performance_csv_size <- 882162600
on_time_performance_rda <- "on_time_performance.rda"
validation_rda <- "validation.rda"
non_validation_rda <- "non_validation.rda"
test_rda <- "test.rda"
train_rda <- "train.rda"
build_models <- FALSE
destination_airport <- "LAX"

```

## Download zip file and check sizes

The script was broken up into sections where the sizes of all the downloaded files were tested before the next step was executed

```{r, echo=TRUE}
#
# if the gz file has not been downloaded or if it is
# the wrong size, download it
#
if ( (!file.exists(on_time_performance_gz)) ||
     (file.info(on_time_performance_gz)$size != on_time_performance_gz_size) ) {

        #
        # Download gz file
        #
        print(paste("file", on_time_performance_gz, 
		"does not exist or is the wrong size", sep=" "))
        print("Downloading it")
        options(timeout = download_timeout)
        download.file(on_time_performance_url, on_time_performance_gz)
        print(paste("Downloaded", file.info(on_time_performance_gz)$size, 
		"bytes"))

        #
        # check to see if the entire file got downloaded
        #
        if ( file.info(on_time_performance_gz)$size == on_time_performance_gz_size) {
                print(paste(on_time_performance_gz, "downloaded OK", sep=" "))
        } else {
                print(paste(on_time_performance_gz, 
			"was not completely downloaded", sep=" "))
                print(paste("You can download it through your browser ",
			"by pointing your browser to"), sep="")
                print(on_time_performance-url)
                stop()
        }
}
```

## Untar file and check sizes

The downloaded file is in gzipped tar format.  The R function *untar* does both a gzip decompress and an untar of the files


```{r echo=TRUE}
#
# if the csv file does not exist or is the wrong size
# or if it is older than the gz file, then untar the
# gz file
#
if ( (!file.exists(on_time_performance_csv)) |
     (file.info(on_time_performance_csv)$size != on_time_performance_csv_size) ||
     (file.info(on_time_performance_gz)$ctime > file.info(on_time_performance_csv)$ctime) ) {

        #
        # untar (inclues gz decompress)
        #
        print(paste("Untaring (will also do a gz decompress)", on_time_performance_gz, sep=" "))
        untar(on_time_performance_gz)

        #
        # check to see if the files sizes are correct
        #
        if ( file.info(on_time_performance_csv)$size == on_time_performance_csv_size) {
                print(paste(on_time_performance_csv, "untared OK", sep=" "))
        } else {
                print(paste(on_time_performance_csv, 
			"was not untarred correctly", sep=" "))
                print(paste("You can try decrompressing and untaring ",
			"it your self and rerunning this script"), sep="")
                stop()
        }
}

```

\newpage
# Extracting and Creating datasets

The original downloaded file contained numerous pieces of information.

```{r, eval=TRUE, echo=TRUE}
csv <- read.csv(on_time_performance_csv, nrows=2)
numCols <- ncol(csv)
headings <- names(csv)
```
There are `r numCols` columns in the original data file.


```{r, eval=FALSE, echo=FALSE}
df %>% knitr::kable(caption="Headings in Original Dataset",
                         label="results") %>% kableExtra::kable_styling(latex_options="hold_position")
```
Many of these columns were empty and many were not relevant to the issue at hand.  The author
followed an interative approach into determining which columns were important.  An additional
boolean column was added which indicated whether or not the flight was late in arriving.

```{r, eval=TRUE, echo=TRUE}
#
# if the on_time_performance_rda file does not exist,
# or if the csv file is younger than it, then rebuild it
#
if ( (!file.exists(on_time_performance_rda)) ||
     (file.info(on_time_performance_csv)$ctime > file.info(on_time_performance_rda)$ctime) ) {

        #
        # read in the csv
        #
        print("Reading data file")
        csv <- read.csv(on_time_performance_csv)
        print("read csv")

        #
        # filter out only flights landing at destination_airport
        # select the columns which might interest us (human choice)
        # only keep complete cases
        # add a flag for Late
        #
        on_time_performance <- csv %>%
                filter(Dest == destination_airport) %>%
                select(Year, Month, DayofMonth, DayOfWeek,
                        DOT_ID_Reporting_Airline,
                        Flight_Number_Reporting_Airline,
                        OriginAirportID,
                        DepTime, ArrTime, ArrDelay) %>%
                filter(complete.cases(.)) %>%           # NB the full stop
                mutate(Late = ifelse(ArrDelay > 0, 1, 0))

        print("filtered out NAs and selected columns")
        save(on_time_performance, file=on_time_performance_rda)
        print(paste("Saved", on_time_performance_rda))
}
```

From this newly created dataset, four subsets were created.  The first subset was to extract a validation
set which would be put aside and not accessed until the final model was going to be evaluation.  The
second subset was the remaining elements which were not in the validation set.  This second subset (the
non validation dataset), was subdivided into a training set and a testing set.  All four of these
subsets were stored on intermediate R objects.

```{r, eval=TRUE, echo=TRUE}
#
# if validation, train, and test do not exist or if
# one of them is younger than the big rda
#
# make sure that the Late flag is equitably distributed
#
if ( !file.exists(test_rda) || !file.exists(train_rda) || !file.exists(validation_rda) ||
     !file.exists(non_validation_rda) ||
     (file.info(on_time_performance_rda)$ctime > file.info(validation_rda)) ||
     (file.info(on_time_performance_rda)$ctime > file.info(non_validation_rda)) ||
     (file.info(on_time_performance_rda)$ctime > file.info(train_rda)) ||
     (file.info(on_time_performance_rda)$ctime > file.info(test_rda))) {

        set.seed(1, sample.kind="Rounding")
        load(on_time_performance_rda)

        #
        # make validation and non-validation datasets
        #
        validation_index <- createDataPartition(y=on_time_performance$Late, times=1, p=0.1, list=FALSE)
        validation <- on_time_performance[validation_index,]
        non_validation <- on_time_performance[-validation_index,]


        #
        # divide the non-validation dataset into test and train
        #
        test_index <- createDataPartition(y=non_validation$Late, times=1, p=0.1, list=FALSE)
        test <- non_validation[test_index,]
        train <- non_validation[-test_index,]

        #
        # save on intermediate data objects
        #
        save(test, file=test_rda)
        save(train, file=train_rda)
        save(validation, file=validation_rda)
        save(non_validation, file=non_validation_rda)

        #
        # free up memory
        #
        rm(test, train, validation, non_validation, on_time_performance)
}

```

```{r eval=TRUE, echo=FALSE}
load(non_validation_rda)
nrow_non_validation <- nrow(non_validation)
rm(non_validation)
load(validation_rda)
nrow_validation <- nrow(validation)
rm(validation)
load(train_rda)
nrow_train <- nrow(train)
rm(train)
load(test_rda)
nrow_test <- nrow(test)
rm(test)
datasets <- c("Non-validation", "Validation", "Train", "Test")
numRows <- c(nrow_non_validation, nrow_validation, nrow_train, nrow_test)
df <- data.frame(datasets, numRows)
```

The number of rows in each dataset can be seen in Table \@ref(tab:sizes)

\begin{center}
```{r echo=FALSE}
df %>% knitr::kable(caption="Sizes of various datasets",
                         label="sizes") %>% kableExtra::kable_styling(latex_options="hold_position")
```
\end{center}


\newpage
# Exploratory Analysis {#exploration}

All values and graphs in this exploratory section refer only to the
non_validation set.  This includes the training set and test set, but
specifically excludes the validation set which was extracted and saved
for future use.

## Arrival delays

Although the term is called *ArrDelay*, the value can be positive (indicating
an actual delay) or negative (indicating that the flight arrived early).
Statistics about the *ArrDelay* field can be seen in Table Table
\@ref(tab:ArrDelayStats)

\begin{center}
```{r echo=FALSE}
load(non_validation_rda)
Measurement <- c("Minimum", "Maximum", "Mean", "Std Dev")
Value <- c(min(non_validation$ArrDelay), max(non_validation$ArrDelay),
           mean(non_validation$ArrDelay), sd(non_validation$ArrDelay))
df <- data.frame(Measurement, Value)
df %>% knitr::kable(caption="Statistics about arrival delay times",
                         label="ArrDelayStats") %>% kableExtra::kable_styling(latex_options="hold_position")
```
\end{center}

The distribution of arrival delay values can be seen in 
Figure \@ref(fig:distribution-arrival-delays).

```{r, distribution-arrival-delays, fig.cap="Distribution of arrival delays"}
flights_per_delays <- non_validation %>% group_by(ArrDelay) %>%
  summarize(number_of_flights = n(), ArrDelay = first(ArrDelay))
options(scipen=100000)
histogram_graph(x = flights_per_delays$number_of_flights,
                data=flights_per_delays, bins=30) +
  xlab("Arrival delay (minutes)") +
  xlim(min(flights_per_delays$ArrDelay), max(flights_per_delays$ArrDelay))
```
## Years

The data spanned a number of years from `r min(non_validation$Year)` to 
`r max(non_validation$Year)`.  A distribution of the flights through the 
years can be seen in Figure \@ref(fig:distribution-flights-by-year)

```{r, distribution-flights-by-year, fig.cap="Distribution of flights by year"}
flights_per_year <- non_validation %>% group_by(Year) %>%
  summarize(number_of_flights = n(), Year = first(Year))
bar_graph(x = flights_per_year$Year, y = flights_per_year$number_of_flights,
                data=flights_per_year) +
  xlab("Year") +
  ylab("Number of arrivals")
```

# Average arrival delays by year

The average arrival delay by year can be seen in Figure \@ref(fig:average-delay-by-year)

```{r, average-delay-by-year, fig.cap="Average arrival delay by year"}
flights_per_year <- non_validation %>% group_by(Year) %>%
  summarize(average_delay = mean(ArrDelay), Year = first(Year))
bar_graph(x = flights_per_year$Year, y = flights_per_year$average_delay,
                data=flights_per_year) +
  xlab("Year") +
  ylab("Average delay (minutes)")
```

## Average delay by month

The month of the year (indicating the season and possible weather effects) could
influence arrival delays.  


```{r, average-delay-by-month, fig.cap="Average arrival delay by month"}
flights_per_month <- non_validation %>% group_by(Month) %>%
  summarize(average_delay = mean(ArrDelay), Month = first(Month))
bar_graph(x = flights_per_month$Month, y = flights_per_month$average_delay,
                data=flights_per_month) +
  xlab("Month") +
  ylab("Average delay (minutes)") +
  scale_x_continuous(limits=c(1, 12), breaks=1:12,
            labels=c("Jan", "Feb", "Mar", "Apr", "May", "Jun",
                     "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"))
```

## Arrival delay by day of the week

The day of the week could possibly influence arrival times with weekend traffic
geting different than weekday traffic.

```{r, average-delay-by-day, fig.cap="Average arrival delay by day of the week"}
flights_per_day <- non_validation %>% group_by(DayOfWeek) %>%
  summarize(average_delay = mean(ArrDelay), DayOfWeek = first(DayOfWeek))
bar_graph(x = flights_per_day$DayOfWeek, y = flights_per_day$average_delay,
                data=flights_per_day) +
  xlab("Day of the Week") +
  ylab("Average delay (minutes)") +
  scale_x_continuous(limits=c(1, 7), breaks=1:7,
                     labels=c("Sun", "Mon", "Tue", "Wed", "Thu",
                              "Fri", "Sat"))

```

\newpage
# Model Investigations and Results {#model}
\newpage
# Conclusion {#conclusions}

# Acknowledgements {-}

The author would like to thank Professor Irizarry and his team for creating a compelling and
effective course.  The exercises drew a person into the problem and enticed them to investigate
the data.  Thank you.

# References {-}


Irizarry, R.A., 2021, Introduction to Data Science.

Knuth, D.E., 1984. Literate programming. The Computer Journal, 27(2), pp.97-111.

Ljubuncic, I., 2015. Problem-solving in High Performance Computing: A Situational Awareness Approach with Linux. Morgan Kaufmann.

Love, R., 2013. Linux system programming: talking directly to the kernel and C library. O'Reilly Media, Inc.
